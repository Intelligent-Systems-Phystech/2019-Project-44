\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\usepackage{tabularx}
\usepackage{ dsfont }
%\NOREVIEWERNOTES
\title
    {Раннее прогнозирование достаточного объема выборки для обобщенной линейной модели.}
\author
    {Бучнев~В.\,С., Грабовой~А.\,В., Гадаев~Т.\,Т., Стрижов~В.\,В.} % основной список авторов, выводимый в оглавление


\abstract
    {Исследуется проблема снижения затрат на сбор данных, необходимых для построения
адекватной модели. Рассматриваются задачи линейной и логистической моделей. Для решения этих задач требуется, чтобы выборка содержала необходимое число  объектов. Требуется предложить метод вычисления оптимального обьема данных, соблюдая при этом баланс между точностью модели и и трудозатратами при сборе данных. Предпочтительны те методы оценки объемы, которые позволяют строить адекватные модели по выборкам возможно меньшего объема.

\bigskip
\textbf{Ключевые слова}: \emph {Обобщенная линейная модель, размер выборки, байесовские методы, бутстреп}.}
\titleEng
    {JMLDA paper example: file jmlda-example.tex}
\authorEng
    {Author~F.\,S.$^1$, CoAuthor~F.\,S.$^2$, Name~F.\,S.$^2$}
\organizationEng
    {$^1$Organization; $^2$Organization}
\abstractEng
    {This document is an example of paper prepared with \LaTeXe\
    typesetting system and style file \texttt{jmlda.sty}.

    \bigskip
    \textbf{Keywords}: \emph{keyword, keyword, more keywords}.}
\begin{document}
\maketitle
%\linenumbers

\section{Введение}
При планировании эксперимента требуется оценить минимальный объём выборки — число производимых измерений набора показателей или признаков, необходимый для построениие сформулированных условий. 

Существует большое количество оценки размера выборки. Например, тест множителей Лагранжа, тест отношения правдоподобия и тест Вальда. В работах \cite{Self-Mauritsen-1998, Shieh-2000, Shieh-2005} на основе данных методов построена оценка оптимального размера выборки. Основной минус этих методов заключается в том, что статистики, используемые в критериях, имеют асимптотическое распределение и требуют большого обьема выборки.

Существуют также байесовские оценки обьема выборки: критерий средней апостериорной дисперсии, критерий среднего покрытия, критерий средней длины и метод максимизации полезности. Первые три метода требуют анализа некоторой функции эффективности от размера выборки. Используя некоторое решающее правило, по данной функции определяется достаточный объем выборки. Главный минус этих методов заключается в том, что они не позволяют построить аппроксимацию функции эффективности при большем объеме данных. Метод максимизации полезности максимизирует ожидание некоторой функции полезности по обьему выборки. Все эти методы опираются на апостериорное распределение, что требует достаточно большого обьема выборки.

Предлагается исследовать зависимость среднего значения логарифма правдоподобия от размера доступной выборки, а также его дисперсию. В данной работе предлагается использовать не сами функции эффективности, а их аппроксимации. Для этого предлагается использовать аппроксимацию ковариационной матрицы вектора параметров. После чего аппроксимировать данные две зависимости при помощи метода бутстреп. Для вычислительного эксперимента предлагается использовать классические выборки из UCI репозитория и синтетические данные.

\section{Постановка задачи}

Дана выборка размера $m$:

$$
\mathfrak D_m = \{\textbf{x}_i, y_i\}_{i=1}^m,
$$

где $\textbf{x}_i \in \mathbb{R}^{n}$ - вектор признаков, $~y_i \in \mathbb{Y}$.


Предполагается, что выборка $\mathfrak D_m$ не противоречит гипотезе порождения данных: модель, порождающая данные, задается в следующем виде:

$$
y_i = g(\mathbf{x}_i, \mathbf{w}, \beta),
$$
где $\textbf{w} \in \mathbb{W}$ - вектор параметров, $\beta$ - параметр, отвечающий за шум в данных.

Зависимая переменная $y$ аппроксимируется обобщенно линейной моделью:

$$
\hat{y_i} = f(\mathbf{x}_i, \mathbf{w}) = \mu(\mathbf{w}^{\top}\mathbf{x}_i),
$$
где $\mu$ --- функция связи, для модели линейной регрессии:

$$
 \mu(\mathbf{w}^{\top}\mathbf{x}_i) =  \mathbf{w}^{\top}\mathbf{x}_i,
$$
для логистической регрессии:
$$
\mu(\mathbf{w}^{\top}\mathbf{x}_i) = \frac{1}{1 + exp(-\mathbf{w}^{\top}\mathbf{x})}
$$

Рассмотрим параметрическое семейство функций для модели линейной регрессии:
$$
p(y | \textbf{x}, \textbf{w}) \sim \mathcal{N}(\mu(\mathbf{w}^{\top}\mathbf{x}), 1),
$$ 
для модели логистической регрессии:
$$
p(y | \textbf{x}, \textbf{w}) \sim \mathcal{B}e(\mu(\mathbf{w}^{\top}\mathbf{x}))
$$
для аппроксимации неизвестного распределения.

%$$
%\mathfrak{F} = \left\{f(y, \textbf{x}, \textbf{w}) | \textbf{w} \in \mathbb{W}, \int_{y \in \mathbb{Y}, \textbf{x} \in \mathbb{R}^n} f(y, \textbf{x}, \textbf{w})dyd\textbf{x} = 1\right\}.
%$$

Для модели $f$ с вектором параметров $\textbf{w}$ определим функцию правдоподобия и логарифмическую функцию правдоподобия выборки $\mathfrak D_m$:
$$
L(\textbf{w} | \mathfrak D_m) = \prod_{y, \textbf{x} \in \mathfrak D_m} p(y | \textbf{x}, \textbf{w}),~~~ l(\textbf w | \mathfrak D_m) = \sum_{y, \textbf{x} \in \mathfrak D_m}\log p(y | \textbf{x}, \textbf{w}),
$$.

%Рассмотрим правдоподобие выборки $\mathfrak D_{\mathcal L_m}$:

%$$
%L(\mathfrak D_{\mathcal T_m}, \mathfrak D_{\mathcal L_m}) = \prod_{y, \textbf{x} \in \mathfrak D_{\mathcal T_m}} f(y, \textbf{x},  \textbf{w}).
%$$

%Рассмотрим логарифм правдоподобия выборки $\mathfrak D_{\mathcal L_m}$:

%$$
%l(\mathfrak D_{\mathcal T_m}, \textbf w) = \sum_{y, \textbf{x} \in \mathfrak D_{\mathcal T_m}}\log f(y, \textbf{x}, \textbf{w}).
%$$

%Будем рассматривать ожидаемое значение функции $l$:

%$$
%\tilde{l}(\mathfrak D)  = \underset{y, \textbf{x} \in \mathfrak D}{\mathsf E} l(\{y, \textbf{x}\}, \textbf w).
%$$

Для оценки вектора параметров используется принцип максимума правдоподобия:

$$
\hat{\textbf{w}} = \argmax_{\textbf{w} \in \mathbb{W}} L(\textbf{w} | \mathfrak D_{m}).
$$

Для введения понятия достаточности объёма  выборки, рассмотрим ожидаемое значение усредненного логарифма правдоподобия по разным обучающим выборкам $\mathfrak D_{m_0}$ размера $m_0$ на оценке вектора параметров $\hat{\mathbf{w}}$:

$$
l(m^*) = \frac{1}{m^*}\underset{\mathfrak D_{m_0} \subset \mathfrak D_m}{\mathsf E} l(\hat{\mathbf{w}} | \mathfrak D_{m^*}).
$$

Будем считать, что объем выборки достаточный, если:

$$
\forall m_1, m_2 > m^* ~~~ |l(m_1) - l(m_2)| < \delta,
$$
где $\delta$ - достаточно малое пороговое значение.

Для получения аппроксимации функции $l(m_0)$ по выборке $\mathfrak D_{m_0}$ введем процедуру бутстрепа:

\begin{enumerate}
	\item Генерируется случайный вектор индексов $\mathbf{j} = (j_1, ..., j_{m_0})$, где $j_i \sim U(\{1, ..., m_0\})$.
	\item Для полученной выборки $\mathfrak D_{\mathbf{j}} = \{\mathbf{x}_{j_i}, y_{j_i}\}_{i=1}^{m_0}$ считается усредненный логарифм правдоподобия на оценке вектора параметров $\hat{\mathbf{w}}$:
$$
\hat{l}_{\mathbf{j}}(m_0) = \frac{1}{m_0}l(\hat{\mathbf{w}} | \mathfrak D_{\mathbf{j}})
$$
	\item пп. 1-2 повторяются $K$ раз ($K$ достаточно большое (?)), оценка $\hat{l}(m_0)$ функции $l(m_0)$ задается формулой:
$$
\hat{l}(m_0) = \frac{1}{K}\sum\limits_{k=1}^K \hat{l}_{\mathbf{j}_k}(m_0),
$$
где $\mathbf{j}_k$ --- вектор индексов, сгенерированный на итерации $k$.
\end{enumerate}

%Рассмотрим матрицу информации Фишера:

%$$
%\textbf{I}(\mathfrak D, \textbf{w}) = -\nabla\nabla l(\mathfrak D, \textbf{w}).
%$$

%Будем считать, что $\hat{\textbf{w}}$ имеет следующее распределение:

%$$
%\hat{\textbf{w}} \sim \mathcal{N}(\textbf{m}, \textbf{V}),~~~ \textbf{V} = \textbf{I}^{-1}(\mathfrak D, m).
%$$

%Для линейной регрессии $\mathbb{Y} = \mathbb{R}$, где $y$ представимо в виде:

%$$
%y = \textbf{x}^{\top}\textbf{w} + \epsilon,
%$$
%где $\epsilon \sim \mathcal{N}(0, 1)$. 

%Аппроксимация плотности апостериорной вероятности имеет вид:

%$$
%f(y, \textbf{x}, \textbf{w}) = \mathcal{N}(y|\textbf{x}^{\top}\textbf{w}, 1).
%$$ 

%Для логистической регрессии $\mathbb{Y} = \{0, 1\}$, где $y$ является бернуллиевской случайной величиной:

%$$
%y \sim \mathcal{B}e(\theta),
%$$
%где $\theta$ - неизвестный параметр распределения. Аппроксимация плотности апостериорной вероятности имеет вид:

%$$
%f(y, \textbf{x}, \textbf{w}) = \mathcal{B}e(y|\theta),~~~ \theta = \frac{1}{1 + exp(-\textbf{x}^{\top}\textbf{w})}.
%$$

\section{Анализ эффективности модели}

Методы байесовских оценок объема выборки основаны на ограничении некоторой выбранной характеристики модели. Для анализа эффективности вводится функция от объема выборки, увеличение значений которой интерпретируется как уменьшение эффективности модели. Объем выборки $m^{*}$ выбирается таким, при котором исследуемая функция не превышает некоторого порогового значения $\epsilon$.

\subsection{Критерий средней длины (ALC)}

Пусть $A(\mathfrak{D}) \subset \mathbb{R}^n$ --- множество значений параметров модели \textbf{w}:

$$
A(\mathfrak{D}) = \{\textbf{w} : ||\textbf{w}- \hat{\textbf{w}}|| \leqslant r_m\}
$$
такое, что
$$
\mathsf{P}(A(\mathfrak{D})) = 1 - \alpha,
$$
где $\alpha$ --- некоторое малое значение. 

Критерий средней длины выглядит следующим образом:

$$
\forall m \geqslant  m^{*}~\mathsf{E}_{\mathfrak D_m} r_m \leqslant l , 
$$
где $r_m$ --- радиус шара $A(\mathfrak{D}_m)$, $l$ --- некоторое наперед заданное достаточно малое значение.

\subsection{Критерий среднего покрытия (ACC)}

Данный критерий также опирается на покрытие множества значений параметров модели \textbf{w}.

Критерий среднего покрытия для определения $m^{*}$:

$$
\forall m \geqslant m^{*}~\mathsf{E}_{\mathfrak D_m} \mathsf{P}\{\textbf{w} \in A(\mathfrak D_m)\} \geqslant 1 - \alpha,
$$
где $\alpha$ --- некоторое малое значение.

\section{Описание алгоритма}

\subsection{Методы байесовских оценок объема выборки.}  Для вычисления функции эффективности в методах ALC и ACC  воспользуемся несмещеностью и состоятельностью оценки $\hat{\textbf{w}}$:

\begin{equation}\label{w_hat_I}
\mathsf{E}\hat{\textbf{w}} = \textbf{m}, ~~~\mathsf{D}\hat{\textbf{w}} = \textbf{I}^{-1}(\mathfrak D_m),
\end{equation}
где $\textbf{I}(\mathfrak D_m)$ --- информационная матрица Фишера:

$$
\textbf{I}(\mathfrak D_m) = \mathsf{E}_{\textbf{w}}\left(\frac{\partial L(\mathfrak D_m, \textbf{w})}{\partial \textbf{w}}\right)^2
$$

Далее используется предположение о распределении оценки вектора параметров:

$$
\hat{\textbf{w}} \sim \mathcal{N}(\textbf{m},  \textbf{I}^{-1}(\mathfrak D_m)),
$$
и с помощью сэмплирования вычисляется приближенное значение $r_m$.

\subsection{Модификация алгоритма}

В случае, когда $m \geq m^*$, воспользуемся свойством информационной матрицы Фишера:

$$
\textbf{I}(\mathfrak D_m) =  m~\textbf{I}(\mathfrak D_1).
$$

Получаем аппроксимацию матрицы Фишера для $m$ наблюдений:

$$
\hat{\textbf{I}}(\mathfrak D_m) = \frac{m}{m^{*}} ~\textbf{I}(\mathfrak D_{m^{*}}).
$$

Матрица $\hat{\textbf{I}}(\mathfrak D_m)^{-1}$  является ковариационной матрицой оценки вектора параметров для $m$ наблюдений.

Таким образом, построена аппроксимация параметров распределения $\hat{\textbf{w}}$ для $m$ наблюдений для вычисления приближенного значения функции эффективности:

$$
\hat{\textbf{w}} \sim \mathcal{N}(\textbf{m},  \frac{m}{m^{*}}~\textbf{I}^{-1}(\mathfrak D_{m^{*}})).
$$

\section{Эксперимент на синтетической выборке}

Рассматривается такая выборка $\mathbf{x}_1, ..., \mathbf{x}_n$, что компоненты вектора $x_i \in \mathds{R}^n$ --- реализации случайной величины из распределения  $\mathcal{N}(0, 1)$, 

$$
\epsilon_1, ..., \epsilon_m \sim \mathcal{N}(0, 1).
$$

Решается задача линейной регрессии с целевой переменной 
$$
y_i =  \textbf{w}^{\top}\mathbf{x}_i + \epsilon_i
$$

и функцией ошибки: 

\begin{equation}\label{MSE}
MSE(\mathbf{x}, y, \textbf{w}) = \frac{1}{m}\sum\limits_{i=1}^{m}(y_i - \textbf{w}^{\top}\mathbf{x}_i )^2.
\end{equation}

где $\mathbf{w}$ --- реализация случайной величины из многомерного равномерного  распределения $\mathcal{U}(\mathbf{0}, \mathbf{1})$.

Оценка параметра $\hat{\mathbf{w}}$ определяется следующим образом:


\begin{equation}\label{argminMSE}
\hat{\mathbf{w}} = \argmin_{\mathbf{w}} MSE(\mathbf{x}, y, \mathbf{w}).
\end{equation}


На рис. 1 отображена зависимость значения функции ошибки от объема выборки. Видно, что при достаточно больших $m$ значение функции ошибки почти не меняется, и увеличение объёма выборки не приводит к улучшению модели.

Зависимость посчитана с помощью метода бутстреп:

\begin{enumerate}
	\item Генерируются случайные непересекающиеся подвыборки $\mathfrak D_{\mathcal{L}}, \mathfrak D_{\mathcal{T}} \subset \mathfrak D$, $|\mathfrak D_{\mathcal{L}}| = m,~ |\mathfrak D_{\mathcal{T}}| = 50$.
	\item Вычисляется оценка вектора параметров $\hat{\mathbf{w}}$ как решение оптимизационной задачи (\ref{argminMSE}) на выборке $\mathfrak D_{\mathcal{L}}$.
	\item Считается функция ошибки (\ref{MSE}) на оценке вектора параметров $\hat{\mathbf{w}}$ на выборке $\mathfrak D_{\mathcal{T}}$:
	\item пп. 1-3 повторяются $K$ раз, итоговая оценка равна среднему значению среди полученных оценок функций ошибок (\ref{MSE}).
\end{enumerate}

На рис. 2 представлена зависимость элементов ковариационной матрицы вектора параметров от объема выборки. [Комментарий?].
Зависимость посчитана с помощью метода бутстреп:
\begin{enumerate}
	\item Генерируется случайная подвыборка $\mathfrak D_{\mathcal{L}} \subset \mathfrak D$, $|\mathfrak D_{\mathcal{L}}| = m$
	\item Вычисляется оценка вектора параметров $\hat{\mathbf{w}}$ как решение оптимизационной задачи (\ref{argminMSE}) на выборке $\mathfrak D_{\mathcal{L}}$.
	\item Вычисляется оценка ковариационной матрицы параметров $\hat{A}$ по формуле:
\begin{equation}\label{Ahat}
\hat{A} =  \frac{1}{n}\mathbf{w}^{\top}\mathbf{w}.
\end{equation}
	\item пп. 1-3 повторяются $K$ раз, итоговая оценка равна среднему значению среди полученных оценок матрицы ковариации (\ref{Ahat}).
\end{enumerate}

На рис. 3 представлена зависимость элементов матрицы Фишера от объема выборки. Оценка информационной матрицы Фишера $\hat{I}$ выражена через оценку ковариационной матрицы по формуле (\ref{w_hat_I}).

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_S.pdf}
\caption{Зависимость значения функции ошибки от объема выборки}
\label{fig1}
\end{figure}

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_W.pdf}
\caption{Зависимость значения элементов ковариационной матрицы от объема выборки}
\label{fig2}
\end{figure}

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_I.pdf}
\caption{Зависимость значения элементов матрицы Фишера от объема выборки}
\label{fig3}
\end{figure}

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_approximation_l.pdf}
\caption{Аппроксимация функции l(m) при $m_0 = 200$}
\label{fig4}
\end{figure}

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_approximation_m_opt_diff.pdf}
\caption{Аппроксимация оптимального размера выборки при разных $m_0$ (по волатильности аппроксимации функции l(m))}
\label{fig5}
\end{figure}

Пусть $n = 10$. Проверим, как меняется качество модели в зависимости от количества используемых признаков. Для отбора нужного количества признаков используем Лассо регрессию с функцией ошибки:

$$
MSE(\mathbf{x}, y, \textbf{w}) = \frac{1}{m}\sum\limits_{i=1}^{m}(y_i - \textbf{w}^{\top}\mathbf{x}_i )^2 + \tau \Vert \mathbf{w}\Vert_1.
$$

Используется тот факт, что Лассо регрессия обнуляет малозначимые признаки, и чем больше коэффициент $\tau$, тем больше признаков обнуляется. Таким образом можно упорядочить признаки по значимости. Для того, чтобы выбрать $n^{\prime}$ признаков с наилучшей функцией ошибки, достаточно взять $n^{\prime}$ самых значимых признаков. 

На рис. 6 построена зависимость среднего значения элементов вектора параметров  от величины $\tau$ при обучении Лассо регрессии на подвыборках размера $m_0 = 100$.

На рис. 7, 8 построена зависимость среднего значения функции ошибки от объёма выборки и от количества используемых признаков.

$\newline$

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_lasso_W.pdf}
\caption{Зависимость значений элементов вектора параметров от величины $\tau$}
\label{fig6}
\end{figure}

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_MSE_n.pdf}
\caption{Зависимость значения функции ошибки от объёма выборки и от количества используемых признаков}
\label{fig7}
\end{figure}

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_MSE_surface.pdf}
\caption{Зависимость значения функции ошибки от объёма выборки и от количества используемых признаков}
\label{fig8}
\end{figure}

\begin{figure}[h!t]\center
\includegraphics[width=1\textwidth]{../data/pics/synthetic_W_from_N.pdf}
\caption{Зависимость значения элементов ковариационной матрицы $\hat{A}$ от количества используемых признаков, $m = 100$.}
\label{fig9}
\end{figure}

\section{Вычислительный эксперимент}

Для анализа точности и эффективности предлагаемого метода был проведен вычислительный эксперимент. В качестве данных использовались выборки, описанные в таблице 1.

\begin{table}[h!]
\begin{center}
\caption{Описание выборок}
\label{table1}
\begin{tabularx}{\textwidth}{|p{1in}|X|X|c|}
\hline
	\centering Выборка &\centeringТип задачи&\centering Размер выборки& Число признаков\\
	\hline
	Servo &регрессия&\centering167&4\\
	\hline
	Boston &регрессия&\centering506&13\\
	\hline
	Diabetes&регрессия&\centering 442&5\\
\hline
\end{tabularx}
\end{center}
\end{table}

В ходе эксперимента были модифицированы критерии средней длины и среднего покрытия для линейной регрессии, а именно была построена аппроксимация функции эффективности при большем объеме выборки при помощи аппроксимации ковариационной матрицы вектора параметров через матрицу информации Фишера.

На графике 1 показана зависимость значения функции эффективности от объема выборки для разных методов при разных данных. Синим цветом обозначено посчитанное значение при данном объеме, красным - аппроксимация при подвыборке фиксированного размера. Реальное значение функции эффективности попадает в доверительный интервал, что говорит о работоспособности предлагаемого метода.

\begin{figure}[h!t]\center
\centering\begin{tabular}{@{}c@{ }c@{ }c@{}}
\textbf{ALC метод} & \textbf{ACC метод}\\
\subfloat[Servo]{\includegraphics[width=0.5\textwidth]{../data/pics/ALC_modified_servo.pdf}}&
\subfloat[Servo]{\includegraphics[width=0.5\textwidth]{../data/pics/ACC_modified_servo.pdf}}\\
\subfloat[Boston]{\includegraphics[width=0.5\textwidth]{../data/pics/ALC_modified_boston.pdf}}&
\subfloat[Boston]{\includegraphics[width=0.5\textwidth]{../data/pics/ACC_modified_boston.pdf}}\\
\subfloat[Diabetes]{\includegraphics[width=0.5\textwidth]{../data/pics/ALC_modified_diabetes.pdf}}&
\subfloat[Diabetes]{\includegraphics[width=0.5\textwidth]{../data/pics/ACC_modified_diabetes.pdf}}\\
\end{tabular}

\caption{Зависимость значения функции эффективности от объема выборки}
\label{fig100}
\end{figure}

$\newline$
$\newline$
$\newline$
$\newline$
$\newline$
$\newline$
$\newline$

В таблицах 2, 3 представлены результаты предсказания различными методами.


$\newline$

\begin{table}[h]
\begin{center}
\caption{Предсказание достаточного объема выборки, ALC метод}
\label{table2}
\begin{tabularx}{0.7\textwidth}{|p{1in}|X|c|}
\hline
	\centering Выборка & \centering Реальное значение &Предсказание\\
	\hline
	 Servo & \centering не хватает данных & 450\\
	\hline
	Boston & \centering не хватает данных &1370\\
	\hline
	Diabetes & \centering 235 & 240\\
\hline
\end{tabularx}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\caption{Предсказание достаточного объема выборки, ACC метод}
\label{table3}
\begin{tabularx}{0.7\textwidth}{|p{1in}|X|c|}
\hline
	\centering Выборка & \centering Реальное значение &Предсказание\\
	\hline
	 Servo & \centering не хватает данных & 405\\
	\hline
	Boston & \centering не хватает данных &1410\\
	\hline
	Diabetes & \centering 235 &245\\
\hline
\end{tabularx}
\end{center}
\end{table}

\bibliographystyle{unsrt}
\bibliography{jmlda-bib}
\begin{thebibliography}{1}


\bibitem{Self-Mauritsen-1998}
\BibAuthor{S.\,G.\;Self and R.\,H.\;Mauritsen}
\BibTitle{Power/sample size calculations for generalized linear
models }~//
\BibJournal{Biometrics}, 1988.

\bibitem{Shieh-2000}
\BibAuthor{G.\,Shieh}
\BibTitle{On power and sample size calculations for likelihood ratio tests in generalized
linear models}~//
\BibJournal{Biometrics}, 2000.

\bibitem{Shieh-2005}
\BibAuthor{G.\,Shieh}
\BibTitle{On power and sample size calculations for Wald tests in generalized linear
models}~//
\BibJournal{Journal of Statistical Planning and Inference}, 2005.


\bibitem{Rubin-Stern-1998}
\BibAuthor{D.\,B.\;Rubin and H.\,S.\;Stern}
\BibTitle{Sample size determination using posterior predictive
distributions }~//
\BibJournal{Sankhya : The Indian Journal of Statistics Special Issue on Bayesian
Analysis}, 1998.

\bibitem{Qumsiyeh-1998}
\BibAuthor{Maher Qumsiyeh}
\BibTitle{Using the bootstrap for estimation the sample size in statistical
experiments }~//
\BibJournal{Journal of modern applied statistical methods}, 2002.

\end{thebibliography}

\end{document}

%ACC diabetes = 245 ???
%ACC boston = 1390
%ACC servo = 455

%ALC diabetes = ???
%ALC boston = ???
%ALC servo = ???


