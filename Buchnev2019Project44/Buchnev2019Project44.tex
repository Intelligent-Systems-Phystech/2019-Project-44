\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES
\title
    {Раннее прогнозирование достаточного объема выборки для обобщенной линейной модели.}
\author
    {Бучнев~В.\,С., Грабовой~А.\,В., Гадаев~Т.\,Т., Стрижов~В.\,В.} % основной список авторов, выводимый в оглавление


\abstract
    {Исследуется проблема снижения затрат на сбор данных, необходимых для построения
адекватной модели. Рассматриваются задачи обощенной линейной модели. Для решения этих задач требуется, чтобы выборка содержала необходимое число  объектов. Требуется предложить метод вычисления оптимального обьема данных, соблюдая при этом баланс между точностью модели и и трудозатратами при сборе данных. Предпочтительны те методы оценки объемы, которые позволяют строить адекватные модели по выборкам возможно меньшего объема.

\bigskip
\textbf{Ключевые слова}: \emph {Обобщенная линейная модель, размер выборки}.}
\titleEng
    {JMLDA paper example: file jmlda-example.tex}
\authorEng
    {Author~F.\,S.$^1$, CoAuthor~F.\,S.$^2$, Name~F.\,S.$^2$}
\organizationEng
    {$^1$Organization; $^2$Organization}
\abstractEng
    {This document is an example of paper prepared with \LaTeXe\
    typesetting system and style file \texttt{jmlda.sty}.

    \bigskip
    \textbf{Keywords}: \emph{keyword, keyword, more keywords}.}
\begin{document}
\maketitle
%\linenumbers

\section{Введение}
При планировании эксперимента требуется оценить минимальный объём выборки — число производимых измерений набора показателей или признаков, необходимый для построениие сформулированных условий. 

Существует большое количество оценки размера выборки. Например, тест множителей Лагранжа, тест отношения правдоподобия и тест Вальда. В работах \cite{Self-Mauritsen-1998, Shieh-2000, Shieh-2005} на основе данных методов построена оценка оптимального размера выборки. Основной минус этих методов заключается в том, что статистики, используемые в критериях, имеют асимптотическое распределение и требуют большого обьема выборки.

Существуют также байесовские оценки обьема выборки: критерий средней апостериорной дисперсии, критерий среднего покрытия, критерий средней длины и метод максимизации полезности. Первые три метода требуют анализа некоторой функции эффективности от размера выборки. Используя некоторое решающее правило, по данной функции определяется достаточный объем выборки. Метод максимизации полезности максимизирует ожидание некоторой функции полезности по обьему выборки. Все эти методы опираются на апостериорное распределение, что требует достаточно большого обьема выборки.

Предлагается исследовать зависимость среднего значения логарифма правдоподобия от размера доступной выборки, а также его дисперсию. В данной работе предлагается использовать не сами функции эффективности, а их аппроксимации. Для этого предлагается использовать аппроксимацию ковариационной матрицы вектора параметра. После чего аппроксимировать данные две зависимости при помощи метода бутстреп. Для вычислительного эксперимента предлагается использовать классические выборки из UCI репозитория и синтетические данные.

\section{Постановка задачи}

Дана выборка размера m:
$$
\mathfrak D_m = \{\textbf{x}_i, y_i\}_{i=1}^m,
$$
где $\textbf{x}_i \in \mathbb{R}^{n}$ - вектор признаков, $~y_i \in \mathbb{Y}$.

Выборка $\mathfrak D_m$ разбита случайно на обучение и контроль:

$$
\mathfrak D_{\mathcal T_m} = \{\textbf{x}_i, y_i\}_{i \in \mathcal T_m} ~~~ \mathfrak D_{\mathcal L_m} = \{\textbf{x}_i, y_i\}_{i \in \mathcal L_m}, ~~~ \mathcal T_m \sqcup \mathcal L_m = \{1, ..., m\}. 
$$

Предполагается, что выборка $\mathfrak D_m$ не противоречит гипотезе порождения данных.

Рассмотрим параметрическое семейство функций для аппроксимации неизвестного распределения $p(y | \textbf{x}, \textbf{w})$, где $\textbf{w} \in \mathbb{W}$ - вектор параметров:

$$
\mathfrak{F} = \left\{f(y, \textbf{x}, \textbf{w}) | \textbf{w} \in \mathbb{W}, \int_{y \in \mathbb{Y}, \textbf{x} \in \mathbb{R}^n} f(y, \textbf{x}, \textbf{w})dyd\textbf{x} = 1\right\}.
$$

Для модели $f$ с вектором параметров $\textbf{w}$ определим функцию правдоподобия и логарифмическую функцию правдоподобия выборки $\mathfrak D$:
$$
L(\mathfrak{D}, \textbf{w}) = \prod_{y, \textbf{x} \in \mathfrak D} f(y, \textbf{x}, \textbf{w}),~~~ l(\mathfrak D, \textbf w) = \sum_{y, \textbf{x} \in \mathfrak D}\log f(y, \textbf{x}, \textbf{w}),
$$
где $f(y, \textbf{x}, \textbf{w})$ - аппроксимация апостериорной вероятности выборки $\mathfrak D_{\mathcal L_m}$ при заданном векторе параметров $\textbf{w}$.

Рассмотрим правдоподобие выборки $\mathfrak D_{\mathcal L_m}$:

$$
L(\mathfrak D_{\mathcal T_m}, \mathfrak D_{\mathcal L_m}) = \prod_{y, \textbf{x} \in \mathfrak D_{\mathcal T_m}} f(y, \textbf{x},  \textbf{w}).
$$

Рассмотрим логарифм правдоподобия выборки $\mathfrak D_{\mathcal L_m}$:

$$
l(\mathfrak D_{\mathcal T_m}, \textbf w) = \sum_{y, \textbf{x} \in \mathfrak D_{\mathcal T_m}}\log f(y, \textbf{x}, \textbf{w}).
$$

Будем рассматривать ожидаемое значение функции $l$:

$$
\tilde{l}(\mathfrak D)  = \underset{y, \textbf{x} \in \mathfrak D}{\mathsf E} l(\{y, \textbf{x}\}, \textbf w).
$$

Рассмотрим ожидаемое значение логарифма правдоподобия по разным обучающим выборкам $\mathfrak D_{\mathcal L_m}$ размера $m^*$:

$$
l(m^*) = \underset{\mathfrak D_{\mathcal L_m}}{\mathsf E} \tilde{l}(\mathfrak D_{\mathcal L_m}).
$$

Будем считать, что объем выборки достаточный, если:

$$
\forall m_1, m_2 > m^* ~~~ |l(m_1) - l(m_2)| < \epsilon,
$$
где $\epsilon$ - достаточно малое пороговое значение.


Для оценки вектора параметров используется принцип максимума правдоподобия:

$$
\hat{\textbf{w}} = \argmax_{\textbf{w} \in \mathbb{W}} L(\mathfrak D_{\mathcal L_m}, \textbf{w}).
$$

Рассмотри матрицу информации Фишера:

$$
\textbf{I}(\mathfrak D, \textbf{w}) = -\nabla\nabla l(\mathfrak D, \textbf{w}).
$$

Будем считать, что $\hat{\textbf{w}}$ имеет следующее распределение:

$$
\hat{\textbf{w}} \sim \mathcal{N}(\textbf{m}, \textbf{V}),~~~ V = \textbf{I}^{-1}(\mathfrak D, m).
$$

Для линейной регрессии $\mathbb{Y} = \mathbb{R}$, где $y$ представимо в виде:

$$
y = \textbf{x}^{\top}\textbf{w} + \epsilon,
$$
где $\epsilon \sim \mathcal{N}(0, 1)$. 

Аппроксимация плотности апостериорной вероятности имеет вид:

$$
f(y, \textbf{x}, \textbf{w}) = \mathcal{N}(y|\textbf{x}^{\top}\textbf{w}, 1).
$$ 

Для логистической регрессии $\mathbb{Y} = \{0, 1\}$, где $y$ является бернуллиевской случайной величиной:

$$
y \sim \mathcal{B}e(\theta),
$$
где $\theta$ - неизвестный параметр распределения. Аппроксимация плотности апостериорной вероятности имеет вид:

$$
f(y, \textbf{x}, \textbf{w}) = \mathcal{B}e(y|\theta),~~~ \theta = \frac{1}{1 + exp(-\textbf{x}^{\top}\textbf{w})}.
$$

\bibliographystyle{unsrt}
\bibliography{jmlda-bib}
\begin{thebibliography}{1}


\bibitem{Self-Mauritsen-1998}
\BibAuthor{S.\,G.\;Self and R.\,H.\;Mauritsen}
\BibTitle{Power/sample size calculations for generalized linear
models }~//
\BibJournal{Biometrics}, 1988.

\bibitem{Shieh-2000}
\BibAuthor{G.\,Shieh}
\BibTitle{On power and sample size calculations for likelihood ratio tests in generalized
linear models}~//
\BibJournal{Biometrics}, 2000.

\bibitem{Shieh-2005}
\BibAuthor{G.\,Shieh}
\BibTitle{On power and sample size calculations for Wald tests in generalized linear
models}~//
\BibJournal{Journal of Statistical Planning and Inference}, 2005.


\bibitem{Rubin-Stern-1998}
\BibAuthor{D.\,B.\;Rubin and H.\,S.\;Stern}
\BibTitle{Sample size determination using posterior predictive
distributions }~//
\BibJournal{Sankhya : The Indian Journal of Statistics Special Issue on Bayesian
Analysis}, 1998.

\bibitem{Qumsiyeh-1998}
\BibAuthor{Maher Qumsiyeh}
\BibTitle{Using the bootstrap for estimation the sample size in statistical
experiments }~//
\BibJournal{Journal of modern applied statistical methods}, 2002.

\end{thebibliography}

\end{document}
